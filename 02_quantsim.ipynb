{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-25 00:40:51,293 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from Examples.common import image_net_config\n",
    "from Examples.torch.utils.image_net_evaluator import ImageNetEvaluator\n",
    "from Examples.torch.utils.image_net_trainer import ImageNetTrainer\n",
    "from Examples.torch.utils.image_net_data_loader import ImageNetDataLoader\n",
    "from torchvision.models import resnet50\n",
    "from aimet_torch.model_preparer import prepare_model\n",
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_torch.qc_quantize_op import StaticGridQuantWrapper\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NUM = 100\n",
    "DATASET_DIR   = '/data/dataset/ImageNet_small'\n",
    "Calibrate_DIR = '/data/dataset/ImageNet_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(name,module, input, output):\n",
    "    if module not in cached_input_output:\n",
    "        cached_input_output[module] = []\n",
    "    # Meanwhile store data in the RAM.\n",
    "    cached_input_output[module].append((input[0].detach().cpu(), output.detach().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataPipeline:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_val_dataloader() -> torch.utils.data.DataLoader:\n",
    "        \"\"\"\n",
    "        Instantiates a validation dataloader for ImageNet dataset and returns it\n",
    "        \"\"\"\n",
    "        data_loader = ImageNetDataLoader(Calibrate_DIR,\n",
    "                                         image_size=image_net_config.dataset['image_size'],\n",
    "                                         batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                         is_training=False,\n",
    "                                         num_workers=image_net_config.evaluation['num_workers']).data_loader\n",
    "        return data_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(model: torch.nn.Module, use_cuda: bool) -> float:\n",
    "        \"\"\"\n",
    "        Given a torch model, evaluates its Top-1 accuracy on the dataset\n",
    "        :param model: the model to evaluate\n",
    "        :param use_cuda: whether or not the GPU should be used.\n",
    "        \"\"\"\n",
    "        evaluator = ImageNetEvaluator(DATASET_DIR, image_size=image_net_config.dataset['image_size'],\n",
    "                                      batch_size=image_net_config.evaluation['batch_size'],\n",
    "                                      num_workers=image_net_config.evaluation['num_workers'])\n",
    "\n",
    "        return evaluator.evaluate(model, iterations=None, use_cuda=use_cuda)\n",
    "\n",
    "    @staticmethod\n",
    "    def finetune(model: torch.nn.Module, epochs, learning_rate, learning_rate_schedule, use_cuda):\n",
    "        \"\"\"\n",
    "        Given a torch model, finetunes the model to improve its accuracy\n",
    "        :param model: the model to finetune\n",
    "        :param epochs: The number of epochs used during the finetuning step.\n",
    "        :param learning_rate: The learning rate used during the finetuning step.\n",
    "        :param learning_rate_schedule: The learning rate schedule used during the finetuning step.\n",
    "        :param use_cuda: whether or not the GPU should be used.\n",
    "        \"\"\"\n",
    "        trainer = ImageNetTrainer(DATASET_DIR, image_size=image_net_config.dataset['image_size'],\n",
    "                                  batch_size=image_net_config.train['batch_size'],\n",
    "                                  num_workers=image_net_config.train['num_workers'])\n",
    "\n",
    "        trainer.train(model, max_epochs=epochs, learning_rate=learning_rate,\n",
    "                      learning_rate_schedule=learning_rate_schedule, use_cuda=use_cuda)\n",
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    data_loader = ImageNetDataPipeline.get_val_dataloader()\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in data_loader:\n",
    "\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "\n",
    "            batch_cntr += 1\n",
    "            if (batch_cntr * batch_size) > samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "model = prepare_model(model)\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))\n",
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224))\n",
    "dummy_input = torch.rand(1, 3, 224, 224)    # Shape for each ImageNet sample is (3 channels) x (224 height) x (224 width)\n",
    "if use_cuda:    dummy_input = dummy_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quntization 실행\n",
    "sim = QuantizationSimModel(model=model,\n",
    "                           quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                           dummy_input=dummy_input,\n",
    "                           default_output_bw=8,\n",
    "                           default_param_bw=8)\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data, forward_pass_callback_args=use_cuda)\n",
    "os.makedirs('./output/', exist_ok=True)\n",
    "dummy_input = dummy_input.cpu()\n",
    "sim.export(path='./output/', filename_prefix='resnet50_after_qat', dummy_input=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_name=[]\n",
    "for name,m in sim.model.named_modules():    \n",
    "    if name =='': continue       \n",
    "    if not isinstance(m,StaticGridQuantWrapper): continue\n",
    "    if not (isinstance(m._module_to_wrap, torch.nn.Linear) or isinstance(m._module_to_wrap, torch.nn.Conv2d)): continue\n",
    "    module_name.append(name)\n",
    "module_name_fx = [x.replace('.','_') for x in module_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = sim.model\n",
    "m_inout= dict()\n",
    "for node in mm.graph.nodes:    \n",
    "    if node.name not in module_name_fx: continue\n",
    "    prev_node=[]\n",
    "    for arg in node.args:\n",
    "        if not isinstance(arg, torch.fx.Node): continue\n",
    "        prev_node.append((arg.name).replace('_','.'))\n",
    "    next_node=[]\n",
    "    for user in node.users:\n",
    "        next_node.append((user.name).replace('_','.'))\n",
    "    m_inout[(node.name).replace('_','.')] = {'prev':prev_node,'next':next_node}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "['x']\n",
      "['bn1']\n",
      "---------------\n",
      "layer1.0.conv1\n",
      "['maxpool']\n",
      "['layer1.0.bn1']\n",
      "---------------\n",
      "layer1.0.conv2\n",
      "['layer1.0.relu']\n",
      "['layer1.0.bn2']\n",
      "---------------\n",
      "layer1.0.conv3\n",
      "['layer1.0.module.relu.1']\n",
      "['layer1.0.bn3']\n",
      "---------------\n",
      "layer1.0.downsample.0\n",
      "['maxpool']\n",
      "['layer1.0.downsample.1']\n",
      "---------------\n",
      "layer1.1.conv1\n",
      "['layer1.0.module.relu.2']\n",
      "['layer1.1.bn1']\n",
      "---------------\n",
      "layer1.1.conv2\n",
      "['layer1.1.relu']\n",
      "['layer1.1.bn2']\n",
      "---------------\n",
      "layer1.1.conv3\n",
      "['layer1.1.module.relu.1']\n",
      "['layer1.1.bn3']\n",
      "---------------\n",
      "layer1.2.conv1\n",
      "['layer1.1.module.relu.2']\n",
      "['layer1.2.bn1']\n",
      "---------------\n",
      "layer1.2.conv2\n",
      "['layer1.2.relu']\n",
      "['layer1.2.bn2']\n",
      "---------------\n",
      "layer1.2.conv3\n",
      "['layer1.2.module.relu.1']\n",
      "['layer1.2.bn3']\n",
      "---------------\n",
      "layer2.0.conv1\n",
      "['layer1.2.module.relu.2']\n",
      "['layer2.0.bn1']\n",
      "---------------\n",
      "layer2.0.conv2\n",
      "['layer2.0.relu']\n",
      "['layer2.0.bn2']\n",
      "---------------\n",
      "layer2.0.conv3\n",
      "['layer2.0.module.relu.1']\n",
      "['layer2.0.bn3']\n",
      "---------------\n",
      "layer2.0.downsample.0\n",
      "['layer1.2.module.relu.2']\n",
      "['layer2.0.downsample.1']\n",
      "---------------\n",
      "layer2.1.conv1\n",
      "['layer2.0.module.relu.2']\n",
      "['layer2.1.bn1']\n",
      "---------------\n",
      "layer2.1.conv2\n",
      "['layer2.1.relu']\n",
      "['layer2.1.bn2']\n",
      "---------------\n",
      "layer2.1.conv3\n",
      "['layer2.1.module.relu.1']\n",
      "['layer2.1.bn3']\n",
      "---------------\n",
      "layer2.2.conv1\n",
      "['layer2.1.module.relu.2']\n",
      "['layer2.2.bn1']\n",
      "---------------\n",
      "layer2.2.conv2\n",
      "['layer2.2.relu']\n",
      "['layer2.2.bn2']\n",
      "---------------\n",
      "layer2.2.conv3\n",
      "['layer2.2.module.relu.1']\n",
      "['layer2.2.bn3']\n",
      "---------------\n",
      "layer2.3.conv1\n",
      "['layer2.2.module.relu.2']\n",
      "['layer2.3.bn1']\n",
      "---------------\n",
      "layer2.3.conv2\n",
      "['layer2.3.relu']\n",
      "['layer2.3.bn2']\n",
      "---------------\n",
      "layer2.3.conv3\n",
      "['layer2.3.module.relu.1']\n",
      "['layer2.3.bn3']\n",
      "---------------\n",
      "layer3.0.conv1\n",
      "['layer2.3.module.relu.2']\n",
      "['layer3.0.bn1']\n",
      "---------------\n",
      "layer3.0.conv2\n",
      "['layer3.0.relu']\n",
      "['layer3.0.bn2']\n",
      "---------------\n",
      "layer3.0.conv3\n",
      "['layer3.0.module.relu.1']\n",
      "['layer3.0.bn3']\n",
      "---------------\n",
      "layer3.0.downsample.0\n",
      "['layer2.3.module.relu.2']\n",
      "['layer3.0.downsample.1']\n",
      "---------------\n",
      "layer3.1.conv1\n",
      "['layer3.0.module.relu.2']\n",
      "['layer3.1.bn1']\n",
      "---------------\n",
      "layer3.1.conv2\n",
      "['layer3.1.relu']\n",
      "['layer3.1.bn2']\n",
      "---------------\n",
      "layer3.1.conv3\n",
      "['layer3.1.module.relu.1']\n",
      "['layer3.1.bn3']\n",
      "---------------\n",
      "layer3.2.conv1\n",
      "['layer3.1.module.relu.2']\n",
      "['layer3.2.bn1']\n",
      "---------------\n",
      "layer3.2.conv2\n",
      "['layer3.2.relu']\n",
      "['layer3.2.bn2']\n",
      "---------------\n",
      "layer3.2.conv3\n",
      "['layer3.2.module.relu.1']\n",
      "['layer3.2.bn3']\n",
      "---------------\n",
      "layer3.3.conv1\n",
      "['layer3.2.module.relu.2']\n",
      "['layer3.3.bn1']\n",
      "---------------\n",
      "layer3.3.conv2\n",
      "['layer3.3.relu']\n",
      "['layer3.3.bn2']\n",
      "---------------\n",
      "layer3.3.conv3\n",
      "['layer3.3.module.relu.1']\n",
      "['layer3.3.bn3']\n",
      "---------------\n",
      "layer3.4.conv1\n",
      "['layer3.3.module.relu.2']\n",
      "['layer3.4.bn1']\n",
      "---------------\n",
      "layer3.4.conv2\n",
      "['layer3.4.relu']\n",
      "['layer3.4.bn2']\n",
      "---------------\n",
      "layer3.4.conv3\n",
      "['layer3.4.module.relu.1']\n",
      "['layer3.4.bn3']\n",
      "---------------\n",
      "layer3.5.conv1\n",
      "['layer3.4.module.relu.2']\n",
      "['layer3.5.bn1']\n",
      "---------------\n",
      "layer3.5.conv2\n",
      "['layer3.5.relu']\n",
      "['layer3.5.bn2']\n",
      "---------------\n",
      "layer3.5.conv3\n",
      "['layer3.5.module.relu.1']\n",
      "['layer3.5.bn3']\n",
      "---------------\n",
      "layer4.0.conv1\n",
      "['layer3.5.module.relu.2']\n",
      "['layer4.0.bn1']\n",
      "---------------\n",
      "layer4.0.conv2\n",
      "['layer4.0.relu']\n",
      "['layer4.0.bn2']\n",
      "---------------\n",
      "layer4.0.conv3\n",
      "['layer4.0.module.relu.1']\n",
      "['layer4.0.bn3']\n",
      "---------------\n",
      "layer4.0.downsample.0\n",
      "['layer3.5.module.relu.2']\n",
      "['layer4.0.downsample.1']\n",
      "---------------\n",
      "layer4.1.conv1\n",
      "['layer4.0.module.relu.2']\n",
      "['layer4.1.bn1']\n",
      "---------------\n",
      "layer4.1.conv2\n",
      "['layer4.1.relu']\n",
      "['layer4.1.bn2']\n",
      "---------------\n",
      "layer4.1.conv3\n",
      "['layer4.1.module.relu.1']\n",
      "['layer4.1.bn3']\n",
      "---------------\n",
      "layer4.2.conv1\n",
      "['layer4.1.module.relu.2']\n",
      "['layer4.2.bn1']\n",
      "---------------\n",
      "layer4.2.conv2\n",
      "['layer4.2.relu']\n",
      "['layer4.2.bn2']\n",
      "---------------\n",
      "layer4.2.conv3\n",
      "['layer4.2.module.relu.1']\n",
      "['layer4.2.bn3']\n",
      "---------------\n",
      "fc\n",
      "['flatten']\n",
      "['output']\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for k in m_inout.keys():\n",
    "    print(k)\n",
    "    print(m_inout[k]['prev'])\n",
    "    print(m_inout[k]['next'])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StaticGridQuantWrapper(\n",
       "  (_module_to_wrap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x,\n",
       " conv1,\n",
       " bn1,\n",
       " relu,\n",
       " maxpool,\n",
       " layer1_0_conv1,\n",
       " layer1_0_bn1,\n",
       " layer1_0_relu,\n",
       " layer1_0_conv2,\n",
       " layer1_0_bn2,\n",
       " layer1_0_module_relu_1,\n",
       " layer1_0_conv3,\n",
       " layer1_0_bn3,\n",
       " layer1_0_downsample_0,\n",
       " layer1_0_downsample_1,\n",
       " layer1_0_module_add,\n",
       " layer1_0_module_relu_2,\n",
       " layer1_1_conv1,\n",
       " layer1_1_bn1,\n",
       " layer1_1_relu,\n",
       " layer1_1_conv2,\n",
       " layer1_1_bn2,\n",
       " layer1_1_module_relu_1,\n",
       " layer1_1_conv3,\n",
       " layer1_1_bn3,\n",
       " layer1_1_module_add_1,\n",
       " layer1_1_module_relu_2,\n",
       " layer1_2_conv1,\n",
       " layer1_2_bn1,\n",
       " layer1_2_relu,\n",
       " layer1_2_conv2,\n",
       " layer1_2_bn2,\n",
       " layer1_2_module_relu_1,\n",
       " layer1_2_conv3,\n",
       " layer1_2_bn3,\n",
       " layer1_2_module_add_2,\n",
       " layer1_2_module_relu_2,\n",
       " layer2_0_conv1,\n",
       " layer2_0_bn1,\n",
       " layer2_0_relu,\n",
       " layer2_0_conv2,\n",
       " layer2_0_bn2,\n",
       " layer2_0_module_relu_1,\n",
       " layer2_0_conv3,\n",
       " layer2_0_bn3,\n",
       " layer2_0_downsample_0,\n",
       " layer2_0_downsample_1,\n",
       " layer2_0_module_add_3,\n",
       " layer2_0_module_relu_2,\n",
       " layer2_1_conv1,\n",
       " layer2_1_bn1,\n",
       " layer2_1_relu,\n",
       " layer2_1_conv2,\n",
       " layer2_1_bn2,\n",
       " layer2_1_module_relu_1,\n",
       " layer2_1_conv3,\n",
       " layer2_1_bn3,\n",
       " layer2_1_module_add_4,\n",
       " layer2_1_module_relu_2,\n",
       " layer2_2_conv1,\n",
       " layer2_2_bn1,\n",
       " layer2_2_relu,\n",
       " layer2_2_conv2,\n",
       " layer2_2_bn2,\n",
       " layer2_2_module_relu_1,\n",
       " layer2_2_conv3,\n",
       " layer2_2_bn3,\n",
       " layer2_2_module_add_5,\n",
       " layer2_2_module_relu_2,\n",
       " layer2_3_conv1,\n",
       " layer2_3_bn1,\n",
       " layer2_3_relu,\n",
       " layer2_3_conv2,\n",
       " layer2_3_bn2,\n",
       " layer2_3_module_relu_1,\n",
       " layer2_3_conv3,\n",
       " layer2_3_bn3,\n",
       " layer2_3_module_add_6,\n",
       " layer2_3_module_relu_2,\n",
       " layer3_0_conv1,\n",
       " layer3_0_bn1,\n",
       " layer3_0_relu,\n",
       " layer3_0_conv2,\n",
       " layer3_0_bn2,\n",
       " layer3_0_module_relu_1,\n",
       " layer3_0_conv3,\n",
       " layer3_0_bn3,\n",
       " layer3_0_downsample_0,\n",
       " layer3_0_downsample_1,\n",
       " layer3_0_module_add_7,\n",
       " layer3_0_module_relu_2,\n",
       " layer3_1_conv1,\n",
       " layer3_1_bn1,\n",
       " layer3_1_relu,\n",
       " layer3_1_conv2,\n",
       " layer3_1_bn2,\n",
       " layer3_1_module_relu_1,\n",
       " layer3_1_conv3,\n",
       " layer3_1_bn3,\n",
       " layer3_1_module_add_8,\n",
       " layer3_1_module_relu_2,\n",
       " layer3_2_conv1,\n",
       " layer3_2_bn1,\n",
       " layer3_2_relu,\n",
       " layer3_2_conv2,\n",
       " layer3_2_bn2,\n",
       " layer3_2_module_relu_1,\n",
       " layer3_2_conv3,\n",
       " layer3_2_bn3,\n",
       " layer3_2_module_add_9,\n",
       " layer3_2_module_relu_2,\n",
       " layer3_3_conv1,\n",
       " layer3_3_bn1,\n",
       " layer3_3_relu,\n",
       " layer3_3_conv2,\n",
       " layer3_3_bn2,\n",
       " layer3_3_module_relu_1,\n",
       " layer3_3_conv3,\n",
       " layer3_3_bn3,\n",
       " layer3_3_module_add_10,\n",
       " layer3_3_module_relu_2,\n",
       " layer3_4_conv1,\n",
       " layer3_4_bn1,\n",
       " layer3_4_relu,\n",
       " layer3_4_conv2,\n",
       " layer3_4_bn2,\n",
       " layer3_4_module_relu_1,\n",
       " layer3_4_conv3,\n",
       " layer3_4_bn3,\n",
       " layer3_4_module_add_11,\n",
       " layer3_4_module_relu_2,\n",
       " layer3_5_conv1,\n",
       " layer3_5_bn1,\n",
       " layer3_5_relu,\n",
       " layer3_5_conv2,\n",
       " layer3_5_bn2,\n",
       " layer3_5_module_relu_1,\n",
       " layer3_5_conv3,\n",
       " layer3_5_bn3,\n",
       " layer3_5_module_add_12,\n",
       " layer3_5_module_relu_2,\n",
       " layer4_0_conv1,\n",
       " layer4_0_bn1,\n",
       " layer4_0_relu,\n",
       " layer4_0_conv2,\n",
       " layer4_0_bn2,\n",
       " layer4_0_module_relu_1,\n",
       " layer4_0_conv3,\n",
       " layer4_0_bn3,\n",
       " layer4_0_downsample_0,\n",
       " layer4_0_downsample_1,\n",
       " layer4_0_module_add_13,\n",
       " layer4_0_module_relu_2,\n",
       " layer4_1_conv1,\n",
       " layer4_1_bn1,\n",
       " layer4_1_relu,\n",
       " layer4_1_conv2,\n",
       " layer4_1_bn2,\n",
       " layer4_1_module_relu_1,\n",
       " layer4_1_conv3,\n",
       " layer4_1_bn3,\n",
       " layer4_1_module_add_14,\n",
       " layer4_1_module_relu_2,\n",
       " layer4_2_conv1,\n",
       " layer4_2_bn1,\n",
       " layer4_2_relu,\n",
       " layer4_2_conv2,\n",
       " layer4_2_bn2,\n",
       " layer4_2_module_relu_1,\n",
       " layer4_2_conv3,\n",
       " layer4_2_bn3,\n",
       " layer4_2_module_add_15,\n",
       " layer4_2_module_relu_2,\n",
       " avgpool,\n",
       " flatten,\n",
       " fc,\n",
       " output]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x,\n",
       " conv1,\n",
       " bn1,\n",
       " relu,\n",
       " maxpool,\n",
       " layer1_0_conv1,\n",
       " layer1_0_bn1,\n",
       " layer1_0_relu,\n",
       " layer1_0_conv2,\n",
       " layer1_0_bn2,\n",
       " layer1_0_module_relu_1,\n",
       " layer1_0_conv3,\n",
       " layer1_0_bn3,\n",
       " layer1_0_downsample_0,\n",
       " layer1_0_downsample_1,\n",
       " layer1_0_module_add,\n",
       " layer1_0_module_relu_2,\n",
       " layer1_1_conv1,\n",
       " layer1_1_bn1,\n",
       " layer1_1_relu,\n",
       " layer1_1_conv2,\n",
       " layer1_1_bn2,\n",
       " layer1_1_module_relu_1,\n",
       " layer1_1_conv3,\n",
       " layer1_1_bn3,\n",
       " layer1_1_module_add_1,\n",
       " layer1_1_module_relu_2,\n",
       " layer1_2_conv1,\n",
       " layer1_2_bn1,\n",
       " layer1_2_relu,\n",
       " layer1_2_conv2,\n",
       " layer1_2_bn2,\n",
       " layer1_2_module_relu_1,\n",
       " layer1_2_conv3,\n",
       " layer1_2_bn3,\n",
       " layer1_2_module_add_2,\n",
       " layer1_2_module_relu_2,\n",
       " layer2_0_conv1,\n",
       " layer2_0_bn1,\n",
       " layer2_0_relu,\n",
       " layer2_0_conv2,\n",
       " layer2_0_bn2,\n",
       " layer2_0_module_relu_1,\n",
       " layer2_0_conv3,\n",
       " layer2_0_bn3,\n",
       " layer2_0_downsample_0,\n",
       " layer2_0_downsample_1,\n",
       " layer2_0_module_add_3,\n",
       " layer2_0_module_relu_2,\n",
       " layer2_1_conv1,\n",
       " layer2_1_bn1,\n",
       " layer2_1_relu,\n",
       " layer2_1_conv2,\n",
       " layer2_1_bn2,\n",
       " layer2_1_module_relu_1,\n",
       " layer2_1_conv3,\n",
       " layer2_1_bn3,\n",
       " layer2_1_module_add_4,\n",
       " layer2_1_module_relu_2,\n",
       " layer2_2_conv1,\n",
       " layer2_2_bn1,\n",
       " layer2_2_relu,\n",
       " layer2_2_conv2,\n",
       " layer2_2_bn2,\n",
       " layer2_2_module_relu_1,\n",
       " layer2_2_conv3,\n",
       " layer2_2_bn3,\n",
       " layer2_2_module_add_5,\n",
       " layer2_2_module_relu_2,\n",
       " layer2_3_conv1,\n",
       " layer2_3_bn1,\n",
       " layer2_3_relu,\n",
       " layer2_3_conv2,\n",
       " layer2_3_bn2,\n",
       " layer2_3_module_relu_1,\n",
       " layer2_3_conv3,\n",
       " layer2_3_bn3,\n",
       " layer2_3_module_add_6,\n",
       " layer2_3_module_relu_2,\n",
       " layer3_0_conv1,\n",
       " layer3_0_bn1,\n",
       " layer3_0_relu,\n",
       " layer3_0_conv2,\n",
       " layer3_0_bn2,\n",
       " layer3_0_module_relu_1,\n",
       " layer3_0_conv3,\n",
       " layer3_0_bn3,\n",
       " layer3_0_downsample_0,\n",
       " layer3_0_downsample_1,\n",
       " layer3_0_module_add_7,\n",
       " layer3_0_module_relu_2,\n",
       " layer3_1_conv1,\n",
       " layer3_1_bn1,\n",
       " layer3_1_relu,\n",
       " layer3_1_conv2,\n",
       " layer3_1_bn2,\n",
       " layer3_1_module_relu_1,\n",
       " layer3_1_conv3,\n",
       " layer3_1_bn3,\n",
       " layer3_1_module_add_8,\n",
       " layer3_1_module_relu_2,\n",
       " layer3_2_conv1,\n",
       " layer3_2_bn1,\n",
       " layer3_2_relu,\n",
       " layer3_2_conv2,\n",
       " layer3_2_bn2,\n",
       " layer3_2_module_relu_1,\n",
       " layer3_2_conv3,\n",
       " layer3_2_bn3,\n",
       " layer3_2_module_add_9,\n",
       " layer3_2_module_relu_2,\n",
       " layer3_3_conv1,\n",
       " layer3_3_bn1,\n",
       " layer3_3_relu,\n",
       " layer3_3_conv2,\n",
       " layer3_3_bn2,\n",
       " layer3_3_module_relu_1,\n",
       " layer3_3_conv3,\n",
       " layer3_3_bn3,\n",
       " layer3_3_module_add_10,\n",
       " layer3_3_module_relu_2,\n",
       " layer3_4_conv1,\n",
       " layer3_4_bn1,\n",
       " layer3_4_relu,\n",
       " layer3_4_conv2,\n",
       " layer3_4_bn2,\n",
       " layer3_4_module_relu_1,\n",
       " layer3_4_conv3,\n",
       " layer3_4_bn3,\n",
       " layer3_4_module_add_11,\n",
       " layer3_4_module_relu_2,\n",
       " layer3_5_conv1,\n",
       " layer3_5_bn1,\n",
       " layer3_5_relu,\n",
       " layer3_5_conv2,\n",
       " layer3_5_bn2,\n",
       " layer3_5_module_relu_1,\n",
       " layer3_5_conv3,\n",
       " layer3_5_bn3,\n",
       " layer3_5_module_add_12,\n",
       " layer3_5_module_relu_2,\n",
       " layer4_0_conv1,\n",
       " layer4_0_bn1,\n",
       " layer4_0_relu,\n",
       " layer4_0_conv2,\n",
       " layer4_0_bn2,\n",
       " layer4_0_module_relu_1,\n",
       " layer4_0_conv3,\n",
       " layer4_0_bn3,\n",
       " layer4_0_downsample_0,\n",
       " layer4_0_downsample_1,\n",
       " layer4_0_module_add_13,\n",
       " layer4_0_module_relu_2,\n",
       " layer4_1_conv1,\n",
       " layer4_1_bn1,\n",
       " layer4_1_relu,\n",
       " layer4_1_conv2,\n",
       " layer4_1_bn2,\n",
       " layer4_1_module_relu_1,\n",
       " layer4_1_conv3,\n",
       " layer4_1_bn3,\n",
       " layer4_1_module_add_14,\n",
       " layer4_1_module_relu_2,\n",
       " layer4_2_conv1,\n",
       " layer4_2_bn1,\n",
       " layer4_2_relu,\n",
       " layer4_2_conv2,\n",
       " layer4_2_bn2,\n",
       " layer4_2_module_relu_1,\n",
       " layer4_2_conv3,\n",
       " layer4_2_bn3,\n",
       " layer4_2_module_add_15,\n",
       " layer4_2_module_relu_2,\n",
       " avgpool,\n",
       " flatten,\n",
       " fc,\n",
       " output]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
